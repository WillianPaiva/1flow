# This file is meant to be sourced from a bash-compatible shell.
#
# 1flow example configuration.
#
# Please grep for CHANGE_ME, and change it to suit your need.
#

case "$-" in
    *i*)
        # CHANGE_ME: is usually "PRODUCTION" or "DEVELOPMENT" (just a name).
        echo "——— 1flow ———  CHANGE_ME environment configuration"
        ;;
esac


# —————————————————————————————————————————————————————————— Sparks environment


# Mandatory.
export DJANGO_SETTINGS_MODULE="oneflow.settings"

# CHANGE_ME: Set the password to the same you set during installation.
export SPARKS_PG_SUPERUSER='oneflow_admin'
export SPARKS_PG_SUPERPASS='CHANGE_ME'

# CHANGE_ME: uncomment this line if on OSX.
#export SPARKS_PG_TMPL_DB='template1'


# ————————————————————————————————————————————————— Django Social Auth API keys


# CHANGE_ME: put your twitter keys if you
# plan to enable twitter features and signin.
#
#export TWITTER_CONSUMER_KEY=''
#export TWITTER_CONSUMER_SECRET=''

# CHANGE_ME: put your LinkedIn keys if you
# plan to enable LKIN features and signin.
#
#export LINKEDIN_CONSUMER_KEY=''
#export LINKEDIN_CONSUMER_SECRET=''


# CHANGE_ME: put your Google Oauth2 keys if you
# plan to enable GG features and signin. Change
# the name if you want. Your users will see it.
#
export GOOGLE_DISPLAY_NAME='1flow'
#export GOOGLE_OAUTH2_CLIENT_ID=''
#export GOOGLE_OAUTH2_CLIENT_SECRET=''

# ————————————————————————————————————————————————————————————— Django settings

# CHANGE_ME: if you have a Sentry instance, create one or
# two projects and put the DSNs here. Ask for a 1flow DNS
# if you have a public-common-good installation and if I
# can provide one for you (resources are limited).
#
# NOTE: "?timeout=10" at the end of the DSN is helpful in some cases.
#export RAVEN_DSN=""

# A dedicated DSN for `flower` (celery monitor) and iPython (live shell),
# avoid flooding the core Sentry. This is completely optional.
#export RAVEN_DSN_FLOWER=""

# CHANGE_ME: These 3 are used in the next directives.
# DBCACHE_SERVER is checked by Django settings.

export DBCACHE_SERVER='127.0.0.1'
export DATA_SERVER='127.0.0.1'
export ARCHIVE_SERVER='127.0.0.1'

# This is a memcached instance
export CACHE_DEFAULT="${DBCACHE_SERVER}:11211"

# CHANGE_ME: "oneflow" is the name of both PG and Mongo databases.
# The PG DNS password will be checked set by sparks at each run.
# Just change it here if you want to change it on every PG machine
# of your setup.
export DATABASE_NAME="oneflow_dev"

export DATABASE_USERNAME="oneflow"
export DATABASE_PASSWORD="CHANGE_ME"
export DATABASE_URL="postgres://${DATABASE_USERNAME}:${DATABASE_PASSWORD}@${DBCACHE_SERVER}/${DATABASE_NAME}"

# Default: None for unlimited persistent connections. If you have trouble with
# this (low trafic sites), export this variable with either "0" or any positive
# integer value (see https://docs.djangoproject.com/en/1.6/ref/databases/#persistent-database-connections for details).
# export DATABASE_CONN_MAX_AGE=""


export MONGODB_HOST="${DATA_SERVER}"
export MONGODB_NAME="${DATABASE_NAME}"
export MONGODB_PORT="27017"

# The archive DB will grow much faster than the production database,
# But you can empty it at will. It's here to be able to be able to
# run new feature on past data.
export MONGODB_NAME_ARCHIVE="${DATABASE_NAME}_archive"

# This is just an example of what you can
# do for dynamic multi-host configuration.
#
# if [[ -n "${ON_AUXILLIARY_SERVER}" ]]; then
#     export MONGODB_HOST_ARCHIVE="CHANGE_ME"
#     export MONGODB_PORT_ARCHIVE="27018"
# else
    export MONGODB_HOST_ARCHIVE="${ARCHIVE_SERVER}"
    export MONGODB_PORT_ARCHIVE="27017"
# fi


#
# REDIS databases. See `docs/Infrastructure.md`
# 1flow occupies at least 5 REDIS databases, for celery,
# constance, cached descriptors, tests...
#

export REDIS_PORT="6379"
export REDIS_DB="0"
export REDIS_CELERY_DB="1"

export CONSTANCE_REDIS_CONNECTION="redis://${DBCACHE_SERVER}:${REDIS_PORT}/${REDIS_DB}"

export BROKER_URL="redis://${DBCACHE_SERVER}:${REDIS_PORT}/${REDIS_CELERY_DB}"

# DB #2 holds sessions
export REDIS_SESSIONS_DB="${DBCACHE_SERVER}:${REDIS_PORT}:2"

# optional: the default session timeout
#export REDIS_SESSIONS_TIMEOUT="604800"

export REDIS_DESCRIPTORS_DB="3"
export REDIS_FEEDBACK_DB="4"

# DB #5 is the Django cache; kept in a separate DB to
# be able to flush it quickly and easily if needed:
#    redis-cli -n 5 -r 1 del '*'
export REDIS_CACHE_DB="${DBCACHE_SERVER}:${REDIS_PORT}:5"

# optional: the default cache timeout
#export REDIS_CACHE_TIMEOUT="604800"

export REDIS_TEST_DB="9"

# ————————————————————————————————————————————————————————————— StatsD settings

# CHANGE_ME: set a statsd host if you wan nice stats about everything in 1flow.
#
#if [[ -n "${ON_AUXILLIARY_SERVER}" ]]; then
#    export STATSD_HOST="CHANGE_ME"
#else
#    export STATSD_HOST="CHANGE_ME"
#fi

#export STATSD_PORT="8125"

# CHANGE_ME: set it to "production", "test", "whatever",
# in case you've got multiple 1flow instances push stats
# to the same statd server. The default is "1flow" (see
# in settings/snippets/common.py).
#
#export STATSD_PREFIX="CHANGE_ME"


# ——————————————————————————————————————————————————————————————— DUMP settings
# 1flow provides some management commands to dump/load selective data from
# one instance to another. These dumps can be limited to one or more creators
# (username field) and one or more categories (slug field).

# Separate them by commas WITHOUT spaces. The default limiting user is "admin".
# export SELECTIVE_DUMP_1FLOW_USERS = 'admin'

# Same principle, but these are limiting categories. Default: "1flow-stock"
# export SELECTIVE_DUMP_1FLOW_CATEGORIES = '1flow-stock'
