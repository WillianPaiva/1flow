[
{
  "pk": 1, 
  "model": "core.processorcategory", 
  "fields": {
    "short_description_en": "", 
    "rght": 4, 
    "name_fr": "processeurs souches de 1flow", 
    "maintainer": [
      "karmak23"
    ], 
    "short_description_nt": "Tous les processeurs et chaines de traitement livr\u00e9s avec 1flow ont cette cat\u00e9gorie", 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "description_fr": "Les processeurs et chaines de traitement qui sont publi\u00e9s avec chaque version de 1flow sont rassembl\u00e9s dans cette cat\u00e9gorie.\r\n\r\nMerci de ne pas les modifier, et de ne pas mettre les votres dans cette cat\u00e9gorie, car ils pourraient \u00eatre \u00e9cras\u00e9s lors d'un changement de version.", 
    "is_active": true, 
    "description_en": "This category rassembles all processors and chains released with each version of 1flow.\r\n\r\nPlease don't alter them, and don't put any of yours in this category, or they could be overwritten during a version upgrade.", 
    "slug": "1flow-stock", 
    "lft": 1, 
    "user": [
      "karmak23"
    ], 
    "tree_id": 1, 
    "short_description_fr": "All processors and chains released with 1flow have this category", 
    "name_en": "1flow stock processors", 
    "source_address": "", 
    "name_nt": ""
  }
},
{
  "pk": 3, 
  "model": "core.processorcategory", 
  "fields": {
    "short_description_en": "Holds the necessary to individually validate processors and chains behavior", 
    "rght": 3, 
    "name_fr": "tests 1flow", 
    "maintainer": [
      "karmak23"
    ], 
    "short_description_nt": "", 
    "parent": 1, 
    "level": 1, 
    "description_nt": "", 
    "description_fr": "Cette cat\u00e9gorie et son nom auxilliaire (slug) sont r\u00e9serv\u00e9s, comme \u00ab\u00a01fs\u00a0\u00bb.\r\n\r\nVous y trouverez pratiquement toutes les chaines et processeurs de 1flow.\r\n\r\nNous l'utilisons pour effectuer des tests individuels sur certaines instances, et valider le fonctionnement des processeurs et chaines avant de les \u00e9tiquetter \u00ab\u00a0pr\u00eat(e) pour la production\u00a0\u00bb.", 
    "is_active": true, 
    "description_en": "This category and its slug are reserved, like \u201c1fs\u201d. \r\n\r\nYou will find a lot of chains and processors inside. \r\n\r\nWe use it to run tests on instances and validate the processors and chains before tagging them \u201cproduction ready\u201d.", 
    "slug": "1flow-test", 
    "lft": 2, 
    "user": [
      "karmak23"
    ], 
    "tree_id": 1, 
    "short_description_fr": "Contient le n\u00e9cessaire pour valider individuellement le fonctionnement des processeurs et cha\u00eenes", 
    "name_en": "1flow tests", 
    "source_address": "", 
    "name_nt": ""
  }
},
{
  "pk": 2, 
  "model": "core.processorcategory", 
  "fields": {
    "short_description_en": "Related to anything that has \u201ca content\u201d", 
    "rght": 2, 
    "name_fr": "contenu", 
    "maintainer": [
      "karmak23"
    ], 
    "short_description_nt": "", 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "description_fr": "\u00c7a peut \u00eatre le mod\u00e8le ContentItem, ou n'importe quel mod\u00e8le qui h\u00e9rite de lui.\r\n\r\nLes processeurs qui traitent le contenu des articles (c-\u00e0-d `Article.content`) sont r\u00e9f\u00e9renc\u00e9s dans cette cat\u00e9gorie.\r\n\r\nLes erreurs de traitement relatives \u00e0 ces processeurs sont interrogeables gr\u00e2ce \u00e0 cette cat\u00e9gorie.", 
    "is_active": true, 
    "description_en": "Related to the ContentItem model, or any model that inherits from it.\r\n\r\nProcessors that handle articles content (eg. `Article.content`) are referenced in this category.\r\n\r\nProcessing errors related to these processors are queryable via this category.", 
    "slug": "content", 
    "lft": 1, 
    "user": [
      "karmak23"
    ], 
    "tree_id": 2, 
    "short_description_fr": "En relation avec n'importe quoi qui a \u00ab un contenu\u00a0\u00bb", 
    "name_en": "content", 
    "source_address": "", 
    "name_nt": ""
  }
},
{
  "pk": 4, 
  "model": "core.processorcategory", 
  "fields": {
    "short_description_en": "Related to anything that process items metadata", 
    "rght": 2, 
    "name_fr": "M\u00e9ta-donn\u00e9es", 
    "maintainer": [
      "karmak23"
    ], 
    "short_description_nt": "", 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "description_fr": "", 
    "is_active": true, 
    "description_en": "", 
    "slug": "metadata", 
    "lft": 1, 
    "user": [
      "karmak23"
    ], 
    "tree_id": 3, 
    "short_description_fr": "En relation avec n'importe quel traitement de m\u00e9ta-donn\u00e9es", 
    "name_en": "Metadata", 
    "source_address": "", 
    "name_nt": ""
  }
},
{
  "pk": 5, 
  "model": "core.processorcategory", 
  "fields": {
    "short_description_en": "related to processing that occurs before downloading items", 
    "rght": 2, 
    "name_fr": "pr\u00e9paration", 
    "maintainer": [
      "karmak23"
    ], 
    "short_description_nt": "", 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "description_fr": "En relation avec tous les mod\u00e8les.\r\n\r\nLes processeurs de cette cat\u00e9gories modifient les instances avant que tout autre traitement aie lieu.", 
    "is_active": true, 
    "description_en": "Related to any model. Processors in this category alter instances before other processing occurs.\r\n", 
    "slug": "prepare", 
    "lft": 1, 
    "user": [
      "karmak23"
    ], 
    "tree_id": 4, 
    "short_description_fr": "En relation avec les traitements qui ont lieu avant de t\u00e9l\u00e9charger les \u00e9l\u00e9ments", 
    "name_en": "prepare", 
    "source_address": "", 
    "name_nt": ""
  }
},
{
  "pk": 6, 
  "model": "core.processorcategory", 
  "fields": {
    "short_description_en": "Related to the download phase of items entering 1flow", 
    "rght": 2, 
    "name_fr": "t\u00e9l\u00e9chargement", 
    "maintainer": [
      "karmak23"
    ], 
    "short_description_nt": "", 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "description_fr": "", 
    "is_active": true, 
    "description_en": "", 
    "slug": "download", 
    "lft": 1, 
    "user": [
      "karmak23"
    ], 
    "tree_id": 5, 
    "short_description_fr": "En relation avec la phase de t\u00e9l\u00e9chargement des \u00e9l\u00e9ments entrant dans 1flow", 
    "name_en": "download", 
    "source_address": "", 
    "name_nt": ""
  }
},
{
  "pk": 7, 
  "model": "core.processorcategory", 
  "fields": {
    "short_description_en": "Related to abstract processors, that do not do anything by themselves but are used by others", 
    "rght": 2, 
    "name_fr": "abstrait", 
    "maintainer": [
      "karmak23"
    ], 
    "short_description_nt": "", 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "description_fr": "Les conditions d'acceptation communes \u00e0 plusieurs processeurs, les pr\u00e9-requis commun et m\u00eame les codes de traitements communs peuvent \u00eatre rang\u00e9s dans cette cat\u00e9gorie.", 
    "is_active": true, 
    "description_en": "Common accept conditions, common requirements, common processing code can be put in this category.", 
    "slug": "abstract", 
    "lft": 1, 
    "user": [
      "karmak23"
    ], 
    "tree_id": 6, 
    "short_description_fr": "En relation avec les processeurs abstraits, qui ne font rien en eux-m\u00eames mais sont utilis\u00e9s par d'autres", 
    "name_en": "abstract", 
    "source_address": "", 
    "name_nt": ""
  }
},
{
  "pk": 8, 
  "model": "core.processorcategory", 
  "fields": {
    "short_description_en": "Related to cleaning processors, that post-process downloaded HTML to clean it", 
    "rght": 2, 
    "name_fr": "nettoyage", 
    "maintainer": [
      "karmak23"
    ], 
    "short_description_nt": "", 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "description_fr": "", 
    "is_active": true, 
    "description_en": "", 
    "slug": "clean", 
    "lft": 1, 
    "user": [
      "karmak23"
    ], 
    "tree_id": 7, 
    "short_description_fr": "En relation avec les processeurs qui nettoient le code HTML tout frais t\u00e9l\u00e9charg\u00e9", 
    "name_en": "clean", 
    "source_address": "", 
    "name_nt": ""
  }
},
{
  "pk": 9, 
  "model": "core.processorcategory", 
  "fields": {
    "short_description_en": "Related to processors that enhance database items content", 
    "rght": 2, 
    "name_fr": "am\u00e9lioration", 
    "maintainer": [
      "karmak23"
    ], 
    "short_description_nt": "", 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "description_fr": "", 
    "is_active": true, 
    "description_en": "", 
    "slug": "enhancement", 
    "lft": 1, 
    "user": [
      "karmak23"
    ], 
    "tree_id": 8, 
    "short_description_fr": "En relation avec les processeurs qui am\u00e9liorent le contenu des \u00e9l\u00e9ments de la base de donn\u00e9e", 
    "name_en": "enhancement", 
    "source_address": "", 
    "name_nt": ""
  }
},
{
  "pk": 10, 
  "model": "core.processorcategory", 
  "fields": {
    "short_description_en": "Related to processors that help integrate items more in the 1flow network", 
    "rght": 2, 
    "name_fr": "int\u00e9gration", 
    "maintainer": [
      "karmak23"
    ], 
    "short_description_nt": "", 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "description_fr": "", 
    "is_active": true, 
    "description_en": "", 
    "slug": "integration", 
    "lft": 1, 
    "user": [
      "karmak23"
    ], 
    "tree_id": 9, 
    "short_description_fr": "En relation avec les processeurs qui aident \u00e0 mieux int\u00e9grer les \u00e9l\u00e9ments dans le r\u00e9seau 1flow", 
    "name_en": "integration", 
    "source_address": "", 
    "name_nt": ""
  }
},
{
  "pk": 1, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We start from the base conditions.\r\nbase_conditions = get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)\r\n\r\nreturn  base_conditions and instance.content_type in (None, models.CONTENT_TYPES.NONE)\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 1, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "This is a 1flow legacy default basic processor.", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-simple-html-downloader", 
    "categories": [
      1, 
      6
    ], 
    "name": "Simple HTML downloader", 
    "short_description_en": "", 
    "parameters": "{'force_encoding': {'help_en': 'can be \"meta\", \"deep\" or any valid encoding name like \"utf-8\", \"latin9\", etc. See https://github.com/1flow/1flow/wiki/Processing-parameters#simple-html-downloader for details.', 'required': False, 'type': 'string', 'default_en': 'nothing; no encoding forced.'}, 'user_agent': {'help_en': 'a classic user agent string, as seen in any modern web browser.', 'required': False, 'type': 'string', 'default_en': 'Application settings / configuration value'}}", 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "import requests\r\n\r\nfrom oneflow.base.utils import detect_encoding_from_requests_response\r\n\r\ndownload_parameters = parameters.get('download', {})\r\n\r\n\r\n# NOTE:\r\n#   \u201cUser-agent\u201d is the official HTTP Header syntax.\r\n#   \u201cuser_agent\u201d is the processor parameter (a python name for a variable).\r\n\r\n# Get our own copy of defaults.\r\nHEADERS = models.REQUEST_BASE_HEADERS.copy()\r\n\r\n# See if we have headers parameters; `parameters` comes\r\n# from the chained item, not the processor itself.\r\nuser_agent = download_parameters.get('user_agent', HEADERS.get('User-agent', None))\r\nforce_encoding = download_parameters.get('force_encoding', None)\r\n\r\nif user_agent is not None:\r\n    # merge everything into our final headers.\r\n    HEADERS['User-agent'] = user_agent\r\n\r\nresponse = requests.get(instance.url, headers=HEADERS)\r\n\r\ncontent_type = response.headers.get('content-type', u'unspecified')\r\n\r\nold_content = instance.content\r\n\r\nif not content_type.startswith(u'text/html'):\r\n\r\n    if verbose:\r\n        LOGGER.warning(u'html downloader: not dealing with an HTML content, aborting.')\r\n\r\n    return \r\n  \r\nif force_encoding is None:\r\n    encoding = detect_encoding_from_requests_response(response).lower()\r\n\r\n    if verbose:\r\n        LOGGER.info(u'html downloader: detected encoding is \u201c%s\u201d.', \r\n                    encoding)\r\n\r\nelse:\r\n    force_encoding = force_encoding.lower()\r\n\r\n    if force_encoding in ('meta trust', 'meta', 'meta-trust', 'meta_trust'):\r\n        force_encoding = 'meta trust'\r\n        encoding = detect_encoding_from_requests_response(\r\n            response, meta=True).lower()\r\n\r\n    elif force_encoding in ('deep search', 'deep', \r\n                            'search', 'deep-search', 'deep_search'):\r\n        force_encoding = 'deep search'\r\n        encoding = detect_encoding_from_requests_response(\r\n            response, deep=True).lower()\r\n\r\n    else:\r\n        encoding = force_encoding\r\n\r\n    if verbose:\r\n        LOGGER.info(u'html downloader: forced encoding to \u201c%s\u201d%s.', \r\n                    encoding, u' via {0}'.format(force_encoding) \r\n                    if force_encoding in ('meta trust', 'deep search', ) else u'')\r\n\r\n\r\nif type(response.content) != type(unicode()):\r\n\r\n    if verbose:\r\n        LOGGER.info(u'html downloader: content is not unicode, converting on the fly.')\r\n      \r\n    # TODO: if parameter override\r\n    content = unicode(response.content, encoding)\r\n\r\nelse:\r\n  content = response.content\r\n    \r\ninstance.content = content\r\ninstance.content_type = models.CONTENT_TYPES.HTML\r\n\r\nif commit:\r\n    previous_history = instance.history.first()\r\n\r\n    if old_content is None and previous_history:\r\n        # Do not waste space with the version that contained nothing.\r\n        # HEADS UP: this is not the same as save_without_historical_record().\r\n        # \t\t\tthe historical record is done *after* the save, not before.\r\n        #\t\t\tthus, using the `\u2026without\u2026` would just make next processor\r\n        #           overwrite our version, which is exactly what we DON'T want.\r\n        previous_history.delete()\r\n        \r\n    # save our version to distinguish from next contents.\r\n    instance.save()\r\n\r\nwith statsd.pipeline() as spipe:\r\n    spipe.gauge('articles.counts.empty', -1, delta=True)\r\n    spipe.gauge('articles.counts.html', 1, delta=True)\r\n\r\nreturn True\r\n\r\n# HEADS UP: the NotTextHtmlException is not raised anymore: next parsers will\r\n# \t\t\ttest themselves if the content{_type} is HTML or not. Eventually\r\n# \t\t\tif they come after this one, they should detect any other handled\r\n# \t\t\ttype and download it or do anything relevant with it.\r\n#\r\n# raise NotTextHtmlException(u\"Content is not text/html \"\r\n#                            u\"but %s.\" % content_type,\r\n#                            response=response)"
  }
},
{
  "pk": 1, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We start from the base conditions.\r\nbase_conditions = get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)\r\n\r\nreturn  base_conditions and instance.content_type in (None, models.CONTENT_TYPES.NONE)\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 1, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "This is a 1flow legacy default basic processor.", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-simple-html-downloader", 
    "categories": [
      1, 
      6
    ], 
    "name": "Simple HTML downloader", 
    "short_description_en": "", 
    "parameters": "{'force_encoding': {'help_en': 'can be \"meta\", \"deep\" or any valid encoding name like \"utf-8\", \"latin9\", etc. See https://github.com/1flow/1flow/wiki/Processing-parameters#simple-html-downloader for details.', 'required': False, 'type': 'string', 'default_en': 'nothing; no encoding forced.'}, 'user_agent': {'help_en': 'a classic user agent string, as seen in any modern web browser.', 'required': False, 'type': 'string', 'default_en': 'Application settings / configuration value'}}", 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "import requests\r\n\r\nfrom oneflow.base.utils import detect_encoding_from_requests_response\r\n\r\ndownload_parameters = parameters.get('download', {})\r\n\r\n\r\n# NOTE:\r\n#   \u201cUser-agent\u201d is the official HTTP Header syntax.\r\n#   \u201cuser_agent\u201d is the processor parameter (a python name for a variable).\r\n\r\n# Get our own copy of defaults.\r\nHEADERS = models.REQUEST_BASE_HEADERS.copy()\r\n\r\n# See if we have headers parameters; `parameters` comes\r\n# from the chained item, not the processor itself.\r\nuser_agent = download_parameters.get('user_agent', HEADERS.get('User-agent', None))\r\nforce_encoding = download_parameters.get('force_encoding', None)\r\n\r\nif user_agent is not None:\r\n    # merge everything into our final headers.\r\n    HEADERS['User-agent'] = user_agent\r\n\r\nresponse = requests.get(instance.url, headers=HEADERS)\r\n\r\ncontent_type = response.headers.get('content-type', u'unspecified')\r\n\r\nold_content = instance.content\r\n\r\nif not content_type.startswith(u'text/html'):\r\n\r\n    if verbose:\r\n        LOGGER.warning(u'html downloader: not dealing with an HTML content, aborting.')\r\n\r\n    return \r\n  \r\nif force_encoding is None:\r\n    encoding = detect_encoding_from_requests_response(response).lower()\r\n\r\n    if verbose:\r\n        LOGGER.info(u'html downloader: detected encoding is \u201c%s\u201d.', \r\n                    encoding)\r\n\r\nelse:\r\n    force_encoding = force_encoding.lower()\r\n\r\n    if force_encoding in ('meta trust', 'meta', 'meta-trust', 'meta_trust'):\r\n        force_encoding = 'meta trust'\r\n        encoding = detect_encoding_from_requests_response(\r\n            response, meta=True).lower()\r\n\r\n    elif force_encoding in ('deep search', 'deep', \r\n                            'search', 'deep-search', 'deep_search'):\r\n        force_encoding = 'deep search'\r\n        encoding = detect_encoding_from_requests_response(\r\n            response, deep=True).lower()\r\n\r\n    else:\r\n        encoding = force_encoding\r\n\r\n    if verbose:\r\n        LOGGER.info(u'html downloader: forced encoding to \u201c%s\u201d%s.', \r\n                    encoding, u' via {0}'.format(force_encoding) \r\n                    if force_encoding in ('meta trust', 'deep search', ) else u'')\r\n\r\n\r\nif type(response.content) != type(unicode()):\r\n\r\n    if verbose:\r\n        LOGGER.info(u'html downloader: content is not unicode, converting on the fly.')\r\n      \r\n    # TODO: if parameter override\r\n    content = unicode(response.content, encoding)\r\n\r\nelse:\r\n  content = response.content\r\n    \r\ninstance.content = content\r\ninstance.content_type = models.CONTENT_TYPES.HTML\r\n\r\nif commit:\r\n    previous_history = instance.history.first()\r\n\r\n    if old_content is None and previous_history:\r\n        # Do not waste space with the version that contained nothing.\r\n        # HEADS UP: this is not the same as save_without_historical_record().\r\n        # \t\t\tthe historical record is done *after* the save, not before.\r\n        #\t\t\tthus, using the `\u2026without\u2026` would just make next processor\r\n        #           overwrite our version, which is exactly what we DON'T want.\r\n        previous_history.delete()\r\n        \r\n    # save our version to distinguish from next contents.\r\n    instance.save()\r\n\r\nwith statsd.pipeline() as spipe:\r\n    spipe.gauge('articles.counts.empty', -1, delta=True)\r\n    spipe.gauge('articles.counts.html', 1, delta=True)\r\n\r\nreturn True\r\n\r\n# HEADS UP: the NotTextHtmlException is not raised anymore: next parsers will\r\n# \t\t\ttest themselves if the content{_type} is HTML or not. Eventually\r\n# \t\t\tif they come after this one, they should detect any other handled\r\n# \t\t\ttype and download it or do anything relevant with it.\r\n#\r\n# raise NotTextHtmlException(u\"Content is not text/html \"\r\n#                            u\"but %s.\" % content_type,\r\n#                            response=response)"
  }
},
{
  "pk": 2, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "beautifulsoup4==4.3.2\r\n", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the same conditions as the requests HTML content extractor.\r\nif not get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit):\r\n\treturn False\r\n\r\n# The website home must have been downloaded, we\r\n# will use its content to extract description, etc.\r\nif not instance.content_type == models.CONTENT_TYPES.HTML:\r\n    return False\r\n  \r\nslashes_parts = [p for p in instance.url.split(u'/') if p != u'']\r\n\r\nparts_nr = len(slashes_parts)\r\n\r\nif parts_nr > 5:\r\n    # For sure, this is not a bookmark.\r\n    return False\r\n\r\nif parts_nr == 2:\r\n    # This is a simple website link. For sure, a bookmark.\r\n    # eg. we got ['http', 'www.mysite.com']\r\n    return True\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 2, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "This is 1flow historical bookmark extractor.", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-simple-bookmark-extractor", 
    "categories": [
      1, 
      2
    ], 
    "name": "Simple bookmark extractor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\nfrom bs4 import BeautifulSoup\r\n\r\nslashes_parts = [p for p in instance.url.split(u'/') if p != u'']\r\ndomain_dotted = slashes_parts[1]\r\ndomain_dashed = domain_dotted.replace(u'.', u'-')\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\n#\r\n# TODO: generate a snapshot locally of the website and store the image.\r\n#       STOP using this external non-libre image provider.\r\n\r\ninstance.image_url = (u'http://images.screenshots.com/'\r\n                  u'{0}/{1}-small.jpg').format(\r\n    domain_dotted, domain_dashed)\r\n\r\n#\r\n# We use fetched content to get a proper \r\n# website description of the page, if any.\r\n#\r\nif instance.content is not None:\r\n    try:\r\n        soup = BeautifulSoup(instance.content)\r\n\r\n        for meta in soup.find_all('meta'):\r\n            if meta.attrs.get('name', 'none').lower() \\\r\n                    == 'description':\r\n                instance.excerpt = meta.attrs['content']\r\n\r\n    except:\r\n        LOGGER.exception(u'bookmark-extractor: could not get description '\r\n                         u'of imported bookmark %s %s', instance_name,\r\n                         instance_id)\r\n\r\n    else:\r\n        LOGGER.info(u'bookmark-extractor: set %s %s description to \u201c%s\u201d',\r\n                    instance_name, instance_id, instance.excerpt)\r\n        \r\ninstance.content = (\r\n    u'http://images.screenshots.com/{0}/{1}-large.jpg').format(\r\n        domain_dotted, domain_dashed\r\n    )\r\n\r\ninstance.content_type = models.CONTENT_TYPES.BOOKMARK\r\n\r\nif commit:\r\n    # The version is worth saving: we will keep the HTML content in history.\r\n    instance.save()\r\n\r\n#\r\n# TODO: fetch something like http://www.siteencyclopedia.com/{parts[1]}/\r\n#       and put it in the excerpt.\r\n\r\n# HEADS UP: next processor will find the article with a CONTENT_TYPE_FINAL\r\n#           and will not process it. Except that in the case of force=True\r\n#           and a multiple-processors chain, this is override the behaviour\r\n#           and fetch a content while the instance IS a bookmark. Thus we\r\n#           must always raise a StopException to be sure the chain will \r\n#           stop, whatever `force` is.\r\n\r\nraise models.StopProcessingException(\r\n    u'Done setting up bookmark content for {0} {1}.'.format(\r\n        instance_name, instance_id))\r\n\r\n"
  }
},
{
  "pk": 2, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "beautifulsoup4==4.3.2\r\n", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the same conditions as the requests HTML content extractor.\r\nif not get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit):\r\n\treturn False\r\n\r\n# The website home must have been downloaded, we\r\n# will use its content to extract description, etc.\r\nif not instance.content_type == models.CONTENT_TYPES.HTML:\r\n    return False\r\n  \r\nslashes_parts = [p for p in instance.url.split(u'/') if p != u'']\r\n\r\nparts_nr = len(slashes_parts)\r\n\r\nif parts_nr > 5:\r\n    # For sure, this is not a bookmark.\r\n    return False\r\n\r\nif parts_nr == 2:\r\n    # This is a simple website link. For sure, a bookmark.\r\n    # eg. we got ['http', 'www.mysite.com']\r\n    return True\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 2, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "This is 1flow historical bookmark extractor.", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-simple-bookmark-extractor", 
    "categories": [
      1, 
      2
    ], 
    "name": "Simple bookmark extractor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\nfrom bs4 import BeautifulSoup\r\n\r\nslashes_parts = [p for p in instance.url.split(u'/') if p != u'']\r\ndomain_dotted = slashes_parts[1]\r\ndomain_dashed = domain_dotted.replace(u'.', u'-')\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\n#\r\n# TODO: generate a snapshot locally of the website and store the image.\r\n#       STOP using this external non-libre image provider.\r\n\r\ninstance.image_url = (u'http://images.screenshots.com/'\r\n                  u'{0}/{1}-small.jpg').format(\r\n    domain_dotted, domain_dashed)\r\n\r\n#\r\n# We use fetched content to get a proper \r\n# website description of the page, if any.\r\n#\r\nif instance.content is not None:\r\n    try:\r\n        soup = BeautifulSoup(instance.content)\r\n\r\n        for meta in soup.find_all('meta'):\r\n            if meta.attrs.get('name', 'none').lower() \\\r\n                    == 'description':\r\n                instance.excerpt = meta.attrs['content']\r\n\r\n    except:\r\n        LOGGER.exception(u'bookmark-extractor: could not get description '\r\n                         u'of imported bookmark %s %s', instance_name,\r\n                         instance_id)\r\n\r\n    else:\r\n        LOGGER.info(u'bookmark-extractor: set %s %s description to \u201c%s\u201d',\r\n                    instance_name, instance_id, instance.excerpt)\r\n        \r\ninstance.content = (\r\n    u'http://images.screenshots.com/{0}/{1}-large.jpg').format(\r\n        domain_dotted, domain_dashed\r\n    )\r\n\r\ninstance.content_type = models.CONTENT_TYPES.BOOKMARK\r\n\r\nif commit:\r\n    # The version is worth saving: we will keep the HTML content in history.\r\n    instance.save()\r\n\r\n#\r\n# TODO: fetch something like http://www.siteencyclopedia.com/{parts[1]}/\r\n#       and put it in the excerpt.\r\n\r\n# HEADS UP: next processor will find the article with a CONTENT_TYPE_FINAL\r\n#           and will not process it. Except that in the case of force=True\r\n#           and a multiple-processors chain, this is override the behaviour\r\n#           and fetch a content while the instance IS a bookmark. Thus we\r\n#           must always raise a StopException to be sure the chain will \r\n#           stop, whatever `force` is.\r\n\r\nraise models.StopProcessingException(\r\n    u'Done setting up bookmark content for {0} {1}.'.format(\r\n        instance_name, instance_id))\r\n\r\n"
  }
},
{
  "pk": 3, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "html2text==2014.9.25\r\n", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the same conditions as the requests HTML content extractor.\r\nreturn get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit\r\n    ) and instance.content_type == models.CONTENT_TYPES.CLEANED_HTML", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 3, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "This processor just calls instance.fetch_content_text(). It's a transition processor that will be split in 3 when the process chain runs flawlessly.", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-html-to-markdown-converter", 
    "categories": [
      1, 
      2
    ], 
    "name": "HTML to Markdown converter", 
    "short_description_en": "Soup-strainer + newspaper + breadability (1flow legacy)", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\nimport html2text\r\n\r\nCONTENT_TYPES = models.CONTENT_TYPES\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\nif verbose:\r\n    LOGGER.info(u'html-to-markdown: converting %s %s\u2026',\r\n                instance_name, instance_id)\r\n\r\nmd_converter = html2text.HTML2Text()\r\n\r\n# Set sane defaults. body_width > 0 breaks\r\n# some links by inserting \\n inside them.\r\n#\r\n# MD_V1 had [False, False, 78] (=default parameters)\r\nmd_converter.unicode_snob = True\r\nmd_converter.escape_snob  = True\r\nmd_converter.body_width   = 0\r\n\r\ntry:\r\n    # NOTE: everything should stay in Unicode during this call.\r\n    instance.content = md_converter.handle(instance.content)\r\n\r\nexcept:\r\n    LOGGER.exception(u'html-to-markdown: markdown convertion failed '\r\n                     u'for %s %s', instance_name, instance_id)\r\n    raise\r\n\r\ninstance.content_type = CONTENT_TYPES.MARKDOWN\r\n\r\n# OBSOLETE: This is cleared by the chain now. \r\n# Just keeping it until I validate the behaviour.\r\n#\r\n# if instance.content_error:\r\n#     statsd.gauge('articles.counts.content_errors', -1, delta=True)\r\n#     instance.content_error = None\r\n\r\n#\r\n# TODO: word count\r\n#\r\ninstance.postprocess_markdown_links(commit=False, force=force)\r\n\r\nif commit:\r\n    # This is worth a version now that content & content_type changed.\r\n    instance.save()\r\n\r\nif verbose:\r\n    LOGGER.info(u'html-to-markdown: successfully converted %s %s.',\r\n                instance_name, instance_id)\r\n        \r\nwith statsd.pipeline() as spipe:\r\n     spipe.gauge('articles.counts.cleaned', -1, delta=True)\r\n     spipe.gauge('articles.counts.markdown', 1, delta=True)\r\n"
  }
},
{
  "pk": 3, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "html2text==2014.9.25\r\n", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the same conditions as the requests HTML content extractor.\r\nreturn get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit\r\n    ) and instance.content_type == models.CONTENT_TYPES.CLEANED_HTML", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 3, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "This processor just calls instance.fetch_content_text(). It's a transition processor that will be split in 3 when the process chain runs flawlessly.", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-html-to-markdown-converter", 
    "categories": [
      1, 
      2
    ], 
    "name": "HTML to Markdown converter", 
    "short_description_en": "Soup-strainer + newspaper + breadability (1flow legacy)", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\nimport html2text\r\n\r\nCONTENT_TYPES = models.CONTENT_TYPES\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\nif verbose:\r\n    LOGGER.info(u'html-to-markdown: converting %s %s\u2026',\r\n                instance_name, instance_id)\r\n\r\nmd_converter = html2text.HTML2Text()\r\n\r\n# Set sane defaults. body_width > 0 breaks\r\n# some links by inserting \\n inside them.\r\n#\r\n# MD_V1 had [False, False, 78] (=default parameters)\r\nmd_converter.unicode_snob = True\r\nmd_converter.escape_snob  = True\r\nmd_converter.body_width   = 0\r\n\r\ntry:\r\n    # NOTE: everything should stay in Unicode during this call.\r\n    instance.content = md_converter.handle(instance.content)\r\n\r\nexcept:\r\n    LOGGER.exception(u'html-to-markdown: markdown convertion failed '\r\n                     u'for %s %s', instance_name, instance_id)\r\n    raise\r\n\r\ninstance.content_type = CONTENT_TYPES.MARKDOWN\r\n\r\n# OBSOLETE: This is cleared by the chain now. \r\n# Just keeping it until I validate the behaviour.\r\n#\r\n# if instance.content_error:\r\n#     statsd.gauge('articles.counts.content_errors', -1, delta=True)\r\n#     instance.content_error = None\r\n\r\n#\r\n# TODO: word count\r\n#\r\ninstance.postprocess_markdown_links(commit=False, force=force)\r\n\r\nif commit:\r\n    # This is worth a version now that content & content_type changed.\r\n    instance.save()\r\n\r\nif verbose:\r\n    LOGGER.info(u'html-to-markdown: successfully converted %s %s.',\r\n                instance_name, instance_id)\r\n        \r\nwith statsd.pipeline() as spipe:\r\n     spipe.gauge('articles.counts.cleaned', -1, delta=True)\r\n     spipe.gauge('articles.counts.markdown', 1, delta=True)\r\n"
  }
},
{
  "pk": 4, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\nreturn (\r\n\r\n    # TODO: don't we work on any Content* model?\r\n\r\n    # we work on articles\r\n    isinstance(instance, models.Article)\r\n\r\n  \t# Content parsers accept only good articles.\r\n    # As of 20150208, ContentItem doesn't care,\r\n    # and for BaseItem & UrlItem it means already\r\n    # absolute and not duplicate, which is perfect.\r\n    and instance.is_good\r\n\r\n    # Note: as this is a generic parser chain applied to all websites,\r\n    # we cannot act on orphaned articles.\r\n    # \r\n    # Generally speaking, an orphaned article (eg. with no URL) can be\r\n    # processed in some conditions (eg. if the RSS feed brought us some\r\n    # content). But it so tightly depends on the website and the RSS \r\n    # feed that here, we must avoid doing anything, because there would\r\n    # be too much tests to run to be sure we can do something.\r\n    # \r\n    # Thus, we do nothing on orphaned articles as a conservative \r\n    # measure. But in dedicated parsers (eg. chains that apply only on\r\n    # one or a few websites), we would most certainly accept them.\r\n    and not instance.is_orphaned\r\n  \r\n    # and which are not already converted (or conversion is forced again).\r\n    and (\r\n    \tforce \r\n    \tor instance.content_type not in models.CONTENT_TYPES_FINAL\r\n    )\r\n)\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 4, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-article-parse-accept-conditions", 
    "categories": [
      1, 
      2, 
      7
    ], 
    "name": "base accept conditions for article content parsers", 
    "short_description_en": "Accept-only processor for mutualized accept code", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\n# Do not process anything.\r\n\r\nreturn True"
  }
},
{
  "pk": 4, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\nreturn (\r\n\r\n    # TODO: don't we work on any Content* model?\r\n\r\n    # we work on articles\r\n    isinstance(instance, models.Article)\r\n\r\n  \t# Content parsers accept only good articles.\r\n    # As of 20150208, ContentItem doesn't care,\r\n    # and for BaseItem & UrlItem it means already\r\n    # absolute and not duplicate, which is perfect.\r\n    and instance.is_good\r\n\r\n    # Note: as this is a generic parser chain applied to all websites,\r\n    # we cannot act on orphaned articles.\r\n    # \r\n    # Generally speaking, an orphaned article (eg. with no URL) can be\r\n    # processed in some conditions (eg. if the RSS feed brought us some\r\n    # content). But it so tightly depends on the website and the RSS \r\n    # feed that here, we must avoid doing anything, because there would\r\n    # be too much tests to run to be sure we can do something.\r\n    # \r\n    # Thus, we do nothing on orphaned articles as a conservative \r\n    # measure. But in dedicated parsers (eg. chains that apply only on\r\n    # one or a few websites), we would most certainly accept them.\r\n    and not instance.is_orphaned\r\n  \r\n    # and which are not already converted (or conversion is forced again).\r\n    and (\r\n    \tforce \r\n    \tor instance.content_type not in models.CONTENT_TYPES_FINAL\r\n    )\r\n)\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 4, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-article-parse-accept-conditions", 
    "categories": [
      1, 
      2, 
      7
    ], 
    "name": "base accept conditions for article content parsers", 
    "short_description_en": "Accept-only processor for mutualized accept code", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\n# Do not process anything.\r\n\r\nreturn True"
  }
},
{
  "pk": 4, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\nreturn (\r\n\r\n    # TODO: don't we work on any Content* model?\r\n\r\n    # we work on articles\r\n    isinstance(instance, models.Article)\r\n\r\n  \t# Content parsers accept only good articles.\r\n    # As of 20150208, ContentItem doesn't care,\r\n    # and for BaseItem & UrlItem it means already\r\n    # absolute and not duplicate, which is perfect.\r\n    and instance.is_good\r\n\r\n    # Note: as this is a generic parser chain applied to all websites,\r\n    # we cannot act on orphaned articles.\r\n    # \r\n    # Generally speaking, an orphaned article (eg. with no URL) can be\r\n    # processed in some conditions (eg. if the RSS feed brought us some\r\n    # content). But it so tightly depends on the website and the RSS \r\n    # feed that here, we must avoid doing anything, because there would\r\n    # be too much tests to run to be sure we can do something.\r\n    # \r\n    # Thus, we do nothing on orphaned articles as a conservative \r\n    # measure. But in dedicated parsers (eg. chains that apply only on\r\n    # one or a few websites), we would most certainly accept them.\r\n    and not instance.is_orphaned\r\n  \r\n    # and which are not already converted (or conversion is forced again).\r\n    and (\r\n    \tforce \r\n    \tor instance.content_type not in models.CONTENT_TYPES_FINAL\r\n    )\r\n)\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 4, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-article-parse-accept-conditions", 
    "categories": [
      1, 
      2, 
      7
    ], 
    "name": "base accept conditions for article content parsers", 
    "short_description_en": "Accept-only processor for mutualized accept code", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\n# Do not process anything.\r\n\r\nreturn True"
  }
},
{
  "pk": 9, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "require::", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the same conditions as the requests HTML content extractor.\r\nreturn get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 5, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-newspaper-full-processor", 
    "categories": [
      1, 
      2, 
      6
    ], 
    "name": "NewsPaper \u2014 full processor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\n# LOGGER.info(u'Parsing %s', instance)\r\n\r\nimport logging\r\nimport newspaper\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\n# Newspaper logs too much. CRAP.\r\nlogging.disable(logging.WARNING)\r\n\r\ntry:\r\n    article = newspaper.Article(instance.url)\r\n    article.download()\r\n    article.parse()\r\n\r\nfinally:\r\n    logging.disable(logging.NOTSET)\r\n\r\n# LOGGER.info(u'Parsed %s', instance.url)\r\n\r\ntext = article.text.strip()\r\nneeds_save = False\r\n\r\nif text != u'':\r\n    instance.content = text\r\n    instance.content_type = models.CONTENT_TYPES.MARKDOWN\r\n    needs_save = True  \r\n\r\nif article.title and get_processor_by_slug(\r\n    '1fs-article-title-extract-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit):\r\n  instance.name = article.title\r\n\r\nif instance.excerpt is None:\r\n    if article.summary:\r\n        instance.excerpt = article.summary\r\n        needs_save = True\r\n    \r\n    elif article.meta_description:\r\n        instance.excerpt = article.meta_description\r\n        needs_save = True\r\n\r\n    if instance.excerpt and verbose:\r\n        LOGGER.info(u'newspaper-full: set %s %s excerpt.', \r\n                    instance_name, instance_id)\r\n\r\nif instance.language_id is None:\r\n    if article.meta_lang:\r\n        instance.language = models.Language.get_by_code(article.meta_lang)\r\n        needs_save = True\r\n        \r\n        if verbose:\r\n            LOGGER.info(u'newspaper-full: set %s %s language to %s', \r\n                        instance_name, instance_id, instance.language)\r\n\r\nif instance.image_url is None:\r\n    if article.top_image:\r\n        instance.image_url = article.top_image\r\n        needs_save = True\r\n\r\n        if verbose:\r\n            LOGGER.info(u'newspaper-full: set %s %s image URL to %s', \r\n                        instance_name, instance_id, instance.image_url)\r\n\r\nif needs_save and commit:\r\n    instance.save()"
  }
},
{
  "pk": 9, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "require::", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the same conditions as the requests HTML content extractor.\r\nreturn get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 5, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-newspaper-full-processor", 
    "categories": [
      1, 
      2, 
      6
    ], 
    "name": "NewsPaper \u2014 full processor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\n# LOGGER.info(u'Parsing %s', instance)\r\n\r\nimport logging\r\nimport newspaper\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\n# Newspaper logs too much. CRAP.\r\nlogging.disable(logging.WARNING)\r\n\r\ntry:\r\n    article = newspaper.Article(instance.url)\r\n    article.download()\r\n    article.parse()\r\n\r\nfinally:\r\n    logging.disable(logging.NOTSET)\r\n\r\n# LOGGER.info(u'Parsed %s', instance.url)\r\n\r\ntext = article.text.strip()\r\nneeds_save = False\r\n\r\nif text != u'':\r\n    instance.content = text\r\n    instance.content_type = models.CONTENT_TYPES.MARKDOWN\r\n    needs_save = True  \r\n\r\nif article.title and get_processor_by_slug(\r\n    '1fs-article-title-extract-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit):\r\n  instance.name = article.title\r\n\r\nif instance.excerpt is None:\r\n    if article.summary:\r\n        instance.excerpt = article.summary\r\n        needs_save = True\r\n    \r\n    elif article.meta_description:\r\n        instance.excerpt = article.meta_description\r\n        needs_save = True\r\n\r\n    if instance.excerpt and verbose:\r\n        LOGGER.info(u'newspaper-full: set %s %s excerpt.', \r\n                    instance_name, instance_id)\r\n\r\nif instance.language_id is None:\r\n    if article.meta_lang:\r\n        instance.language = models.Language.get_by_code(article.meta_lang)\r\n        needs_save = True\r\n        \r\n        if verbose:\r\n            LOGGER.info(u'newspaper-full: set %s %s language to %s', \r\n                        instance_name, instance_id, instance.language)\r\n\r\nif instance.image_url is None:\r\n    if article.top_image:\r\n        instance.image_url = article.top_image\r\n        needs_save = True\r\n\r\n        if verbose:\r\n            LOGGER.info(u'newspaper-full: set %s %s image URL to %s', \r\n                        instance_name, instance_id, instance.image_url)\r\n\r\nif needs_save and commit:\r\n    instance.save()"
  }
},
{
  "pk": 9, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "require::", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the same conditions as the requests HTML content extractor.\r\nreturn get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 5, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-newspaper-full-processor", 
    "categories": [
      1, 
      2, 
      6
    ], 
    "name": "NewsPaper \u2014 full processor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\n# LOGGER.info(u'Parsing %s', instance)\r\n\r\nimport logging\r\nimport newspaper\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\n# Newspaper logs too much. CRAP.\r\nlogging.disable(logging.WARNING)\r\n\r\ntry:\r\n    article = newspaper.Article(instance.url)\r\n    article.download()\r\n    article.parse()\r\n\r\nfinally:\r\n    logging.disable(logging.NOTSET)\r\n\r\n# LOGGER.info(u'Parsed %s', instance.url)\r\n\r\ntext = article.text.strip()\r\nneeds_save = False\r\n\r\nif text != u'':\r\n    instance.content = text\r\n    instance.content_type = models.CONTENT_TYPES.MARKDOWN\r\n    needs_save = True  \r\n\r\nif article.title and get_processor_by_slug(\r\n    '1fs-article-title-extract-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit):\r\n  instance.name = article.title\r\n\r\nif instance.excerpt is None:\r\n    if article.summary:\r\n        instance.excerpt = article.summary\r\n        needs_save = True\r\n    \r\n    elif article.meta_description:\r\n        instance.excerpt = article.meta_description\r\n        needs_save = True\r\n\r\n    if instance.excerpt and verbose:\r\n        LOGGER.info(u'newspaper-full: set %s %s excerpt.', \r\n                    instance_name, instance_id)\r\n\r\nif instance.language_id is None:\r\n    if article.meta_lang:\r\n        instance.language = models.Language.get_by_code(article.meta_lang)\r\n        needs_save = True\r\n        \r\n        if verbose:\r\n            LOGGER.info(u'newspaper-full: set %s %s language to %s', \r\n                        instance_name, instance_id, instance.language)\r\n\r\nif instance.image_url is None:\r\n    if article.top_image:\r\n        instance.image_url = article.top_image\r\n        needs_save = True\r\n\r\n        if verbose:\r\n            LOGGER.info(u'newspaper-full: set %s %s image URL to %s', \r\n                        instance_name, instance_id, instance.image_url)\r\n\r\nif needs_save and commit:\r\n    instance.save()"
  }
},
{
  "pk": 10, 
  "model": "core.processor", 
  "fields": {
    "rght": 4, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "newspaper==0.0.8\r\n", 
    "short_description_nt": "", 
    "accept_code": "return False", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 6, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-newspaper-common", 
    "categories": [
      1, 
      7
    ], 
    "name": "NewsPaper (common)", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "return True"
  }
},
{
  "pk": 10, 
  "model": "core.processor", 
  "fields": {
    "rght": 4, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "newspaper==0.0.8\r\n", 
    "short_description_nt": "", 
    "accept_code": "return False", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 6, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-newspaper-common", 
    "categories": [
      1, 
      7
    ], 
    "name": "NewsPaper (common)", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "return True"
  }
},
{
  "pk": 11, 
  "model": "core.processor", 
  "fields": {
    "rght": 3, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 2, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 6, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": 10, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-newspaper-title-extractor", 
    "categories": [
      1, 
      4
    ], 
    "name": "Newspaper \u2014 title extractor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 1, 
    "duplicate_status": null, 
    "process_code": ""
  }
},
{
  "pk": 11, 
  "model": "core.processor", 
  "fields": {
    "rght": 3, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 2, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 6, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": 10, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-newspaper-title-extractor", 
    "categories": [
      1, 
      4
    ], 
    "name": "Newspaper \u2014 title extractor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 1, 
    "duplicate_status": null, 
    "process_code": ""
  }
},
{
  "pk": 13, 
  "model": "core.processor", 
  "fields": {
    "rght": 4, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "opengraph==0.5", 
    "short_description_nt": "", 
    "accept_code": "", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 8, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-opengraph-common", 
    "categories": [
      1, 
      7
    ], 
    "name": "OpenGraph (common)", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": ""
  }
},
{
  "pk": 13, 
  "model": "core.processor", 
  "fields": {
    "rght": 4, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "opengraph==0.5", 
    "short_description_nt": "", 
    "accept_code": "", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 8, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-opengraph-common", 
    "categories": [
      1, 
      7
    ], 
    "name": "OpenGraph (common)", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": ""
  }
},
{
  "pk": 14, 
  "model": "core.processor", 
  "fields": {
    "rght": 3, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 2, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We start from the base conditions.\r\nbase_conditions = get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)\r\n\r\nreturn  base_conditions and instance.content_type == models.CONTENT_TYPES.HTML\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 8, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": 13, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-opengraph-article", 
    "categories": [
      1, 
      4
    ], 
    "name": "OpenGraph \u2014 article metadata extractor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 1, 
    "duplicate_status": null, 
    "process_code": "import opengraph\r\n\r\nfrom oneflow.base.utils.http import clean_url\r\nfrom oneflow.base.utils.dateutils import datetime, datetime_extended_parser\r\n\r\n# from https://github.com/erikriver/opengraph\r\n# site_name       => YouTube\r\n# description     => Eric Clapton and Paul McCartney perform George Harrison's \"While My Guitar Gently Weeps\" at the...\r\n# title           => While My Guitar Gently Weeps\r\n# url             => http://www.youtube.com/watch?v=q3ixBmDzylQ\r\n# image           => http://i2.ytimg.com/vi/q3ixBmDzylQ/default.jpg\r\n# video:type      => application/x-shockwave-flash\r\n# video:height    => 224\r\n# video           => http://www.youtube.com/v/q3ixBmDzylQ?version=3&autohide=1\r\n# video:width     => 398\r\n# type            => video\r\n\r\ndef data_ok(data):\r\n  \r\n    if isinstance(data, list):\r\n        return data[0] is not None and data[0].strip() != u''\r\n  \r\n    return data is not None and data.strip() != u''\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\ntry:\r\n    og_article = opengraph.OpenGraph(html=instance.content)\r\n\r\nexcept:\r\n  if verbose:\r\n      LOGGER.exception(u'opengraph: parsing %s %s failed, aborting.',\r\n                       instance_name, instance_id)\r\n  else:\r\n      # Not worth a round trip to sentry in most cases.\r\n      LOGGER.warning(u'opengraph: parsing %s %s failed, aborting.',\r\n                     instance_name, instance_id)\r\n  return\r\n\r\nif not og_article.is_valid():\r\n    LOGGER.error(u'opengraph: invalid OpenGraph data in %s %s, aborting.',\r\n                 instance_name, instance_id)\r\n    return\r\n\r\nneeds_commit = False\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 Title\r\n\r\nname_needs_extraction = get_processor_by_slug(\r\n    '1fs-article-title-extract-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)\r\n\r\nif data_ok(og_article.title) and name_needs_extraction:\r\n\r\n    instance.name = og_article.title\r\n    needs_commit = True\r\n\r\n    if verbose:\r\n        LOGGER.info(u'opengraph: set %s %s name to \u201c%s\u201d.', \r\n                    instance_name, instance_id, instance.name)\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 Date published\r\n# http://ogp.me/#type_article\r\n# \r\n# article:published_time - datetime - When the article was first published.\r\n# article:modified_time - datetime - When the article was last changed.\r\n# article:expiration_time - datetime - When the article is out of date after.\r\n# article:author - profile array - Writers of the article.\r\n# article:section - string - A high-level section name. E.g. Technology\r\n# article:tag - string array - Tag words associated with this article.\r\n# \r\n# http://ogp.me/#type_profile (for author)\r\n\r\nog_pub_time = og_article.get('article__published_time', None)\r\n\r\nif instance.date_published is None and data_ok(og_pub_time):\r\n\r\n    parsed_datetime = datetime_extended_parser(og_pub_time)\r\n  \r\n    if parsed_datetime is None:\r\n        LOGGER.warning(u'OpenGraph article:published_time \u201c%s\u201d is '\r\n                       u'unparseable.', og_pub_time)\r\n  \r\n    else:\r\n        date_published = datetime(*parsed_datetime[:6])\r\n\r\n        instance.date_published = date_published\r\n        needs_commit = True\r\n        LOGGER.info(u'opengraph: set %s %s published date.', \r\n                    instance_name, instance_id)\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 Description\r\n\r\nog_description = og_article.get('description', None)\r\n\r\nif data_ok(og_description) and not data_ok(instance.excerpt):\r\n    instance.excerpt = og_description\r\n    needs_commit = True\r\n\r\n    if verbose:\r\n        LOGGER.info(u'opengraph: set %s %s excerpt.', \r\n                    instance_name, instance_id)\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 Authors\r\n\r\n#\r\n# TODO\r\n# \r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2014\u2014\u2014 Language\r\n\r\nog_language = og_article.get('language', None)\r\n\r\nif data_ok(og_language) and instance.language_id is None:\r\n    instance.language = models.Language.get_by_code(og_language)\r\n    needs_commit = True\r\n\r\n    if verbose:\r\n        LOGGER.info(u'opengraph: set %s %s language to %s.', \r\n                    instance_name, instance_id, instance.language)\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2014\u2014\u2014 Tags\r\n\r\nog_tags = og_article.get('article__tag', None)\r\n\r\nif data_ok(og_tags):\r\n  \r\n    if not isinstance(og_tags, list):\r\n        og_tags = [og_tags]\r\n\r\n    if og_tags and not instance.tags.exists():\r\n        instance.tags.add(*models.SimpleTag.get_tags_set(og_tags, origin=instance))\r\n\r\n        if verbose:\r\n            LOGGER.info(u'opengraph: set %s %s tag(s) to %s.', \r\n                        instance_name, instance_id, u', '.join(og_tags))\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 Front image\r\n\r\nog_image = og_article.get('image', None)\r\n\r\nif data_ok(og_image) and not data_ok(instance.image_url):\r\n  \r\n    if isinstance(og_image, list):\r\n        instance.image_url = clean_url(og_image[0])\r\n\r\n    else:\r\n        instance.image_url = clean_url(og_image)\r\n      \r\n    needs_commit = True\r\n\r\n    if verbose:\r\n        LOGGER.info(u'opengraph: set %s %s image_url to %s.', \r\n                    instance_name, instance_id, instance.image_url)\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Beer\r\n\r\nif needs_commit and commit:\r\n    # As we changed only fields that were previously unset, no need to waste a version.\r\n    instance.save_without_historical_record()"
  }
},
{
  "pk": 14, 
  "model": "core.processor", 
  "fields": {
    "rght": 3, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 2, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We start from the base conditions.\r\nbase_conditions = get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)\r\n\r\nreturn  base_conditions and instance.content_type == models.CONTENT_TYPES.HTML\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 8, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": 13, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-opengraph-article", 
    "categories": [
      1, 
      4
    ], 
    "name": "OpenGraph \u2014 article metadata extractor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 1, 
    "duplicate_status": null, 
    "process_code": "import opengraph\r\n\r\nfrom oneflow.base.utils.http import clean_url\r\nfrom oneflow.base.utils.dateutils import datetime, datetime_extended_parser\r\n\r\n# from https://github.com/erikriver/opengraph\r\n# site_name       => YouTube\r\n# description     => Eric Clapton and Paul McCartney perform George Harrison's \"While My Guitar Gently Weeps\" at the...\r\n# title           => While My Guitar Gently Weeps\r\n# url             => http://www.youtube.com/watch?v=q3ixBmDzylQ\r\n# image           => http://i2.ytimg.com/vi/q3ixBmDzylQ/default.jpg\r\n# video:type      => application/x-shockwave-flash\r\n# video:height    => 224\r\n# video           => http://www.youtube.com/v/q3ixBmDzylQ?version=3&autohide=1\r\n# video:width     => 398\r\n# type            => video\r\n\r\ndef data_ok(data):\r\n  \r\n    if isinstance(data, list):\r\n        return data[0] is not None and data[0].strip() != u''\r\n  \r\n    return data is not None and data.strip() != u''\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\ntry:\r\n    og_article = opengraph.OpenGraph(html=instance.content)\r\n\r\nexcept:\r\n  if verbose:\r\n      LOGGER.exception(u'opengraph: parsing %s %s failed, aborting.',\r\n                       instance_name, instance_id)\r\n  else:\r\n      # Not worth a round trip to sentry in most cases.\r\n      LOGGER.warning(u'opengraph: parsing %s %s failed, aborting.',\r\n                     instance_name, instance_id)\r\n  return\r\n\r\nif not og_article.is_valid():\r\n    LOGGER.error(u'opengraph: invalid OpenGraph data in %s %s, aborting.',\r\n                 instance_name, instance_id)\r\n    return\r\n\r\nneeds_commit = False\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 Title\r\n\r\nname_needs_extraction = get_processor_by_slug(\r\n    '1fs-article-title-extract-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)\r\n\r\nif data_ok(og_article.title) and name_needs_extraction:\r\n\r\n    instance.name = og_article.title\r\n    needs_commit = True\r\n\r\n    if verbose:\r\n        LOGGER.info(u'opengraph: set %s %s name to \u201c%s\u201d.', \r\n                    instance_name, instance_id, instance.name)\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 Date published\r\n# http://ogp.me/#type_article\r\n# \r\n# article:published_time - datetime - When the article was first published.\r\n# article:modified_time - datetime - When the article was last changed.\r\n# article:expiration_time - datetime - When the article is out of date after.\r\n# article:author - profile array - Writers of the article.\r\n# article:section - string - A high-level section name. E.g. Technology\r\n# article:tag - string array - Tag words associated with this article.\r\n# \r\n# http://ogp.me/#type_profile (for author)\r\n\r\nog_pub_time = og_article.get('article__published_time', None)\r\n\r\nif instance.date_published is None and data_ok(og_pub_time):\r\n\r\n    parsed_datetime = datetime_extended_parser(og_pub_time)\r\n  \r\n    if parsed_datetime is None:\r\n        LOGGER.warning(u'OpenGraph article:published_time \u201c%s\u201d is '\r\n                       u'unparseable.', og_pub_time)\r\n  \r\n    else:\r\n        date_published = datetime(*parsed_datetime[:6])\r\n\r\n        instance.date_published = date_published\r\n        needs_commit = True\r\n        LOGGER.info(u'opengraph: set %s %s published date.', \r\n                    instance_name, instance_id)\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 Description\r\n\r\nog_description = og_article.get('description', None)\r\n\r\nif data_ok(og_description) and not data_ok(instance.excerpt):\r\n    instance.excerpt = og_description\r\n    needs_commit = True\r\n\r\n    if verbose:\r\n        LOGGER.info(u'opengraph: set %s %s excerpt.', \r\n                    instance_name, instance_id)\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 Authors\r\n\r\n#\r\n# TODO\r\n# \r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2014\u2014\u2014 Language\r\n\r\nog_language = og_article.get('language', None)\r\n\r\nif data_ok(og_language) and instance.language_id is None:\r\n    instance.language = models.Language.get_by_code(og_language)\r\n    needs_commit = True\r\n\r\n    if verbose:\r\n        LOGGER.info(u'opengraph: set %s %s language to %s.', \r\n                    instance_name, instance_id, instance.language)\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2014\u2014\u2014 Tags\r\n\r\nog_tags = og_article.get('article__tag', None)\r\n\r\nif data_ok(og_tags):\r\n  \r\n    if not isinstance(og_tags, list):\r\n        og_tags = [og_tags]\r\n\r\n    if og_tags and not instance.tags.exists():\r\n        instance.tags.add(*models.SimpleTag.get_tags_set(og_tags, origin=instance))\r\n\r\n        if verbose:\r\n            LOGGER.info(u'opengraph: set %s %s tag(s) to %s.', \r\n                        instance_name, instance_id, u', '.join(og_tags))\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 Front image\r\n\r\nog_image = og_article.get('image', None)\r\n\r\nif data_ok(og_image) and not data_ok(instance.image_url):\r\n  \r\n    if isinstance(og_image, list):\r\n        instance.image_url = clean_url(og_image[0])\r\n\r\n    else:\r\n        instance.image_url = clean_url(og_image)\r\n      \r\n    needs_commit = True\r\n\r\n    if verbose:\r\n        LOGGER.info(u'opengraph: set %s %s image_url to %s.', \r\n                    instance_name, instance_id, instance.image_url)\r\n\r\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Beer\r\n\r\nif needs_commit and commit:\r\n    # As we changed only fields that were previously unset, no need to waste a version.\r\n    instance.save_without_historical_record()"
  }
},
{
  "pk": 15, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\nORIGINS = models.ORIGINS\r\n\r\nreturn (\r\n\t# If instance has no name currently, wherever its origin is, \r\n\t# it seems fine to try to find a name from any data we have\r\n\t# or can gather.\r\n\r\n\tinstance.name is None\r\n\tor \r\n\tinstance.name.strip() == u''\r\n\r\n\t# Or, generally speaking, RSS already sends us the title, \r\n\t# thus we should not do the job twice if we already have it.\r\n\tor (\r\n\t\tinstance.origin not in (\r\n\t\t\tORIGINS.FEEDPARSER,\r\n\t\t\tORIGINS.GOOGLE_READER,\r\n\t\t)\r\n\r\n\t\t# In case the origin is not one known to give us the title,\r\n\t\t# We should check if the title is not already extracted to \r\n\t\t# not do the job twice again.\r\n\r\n\t\tand (\r\n\t        instance.name.endswith(instance.url)\r\n\r\n\t        # Except if the user, the admin or the \r\n\t        # engine explicitely asks us to do so.\r\n\t        \r\n        \tor\r\n        \tforce\r\n        )\r\n\t) \r\n)", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 9, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-article-title-extract-accept-conditions", 
    "categories": [
      1, 
      4, 
      7
    ], 
    "name": "base accept conditions for article title extraction", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\n# This processor does NOT extract any title from anything.\r\n# It just defines a common accepts() conditions set that \r\n# other title extractor processors can start from.\r\n\r\nreturn True"
  }
},
{
  "pk": 15, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\nORIGINS = models.ORIGINS\r\n\r\nreturn (\r\n\t# If instance has no name currently, wherever its origin is, \r\n\t# it seems fine to try to find a name from any data we have\r\n\t# or can gather.\r\n\r\n\tinstance.name is None\r\n\tor \r\n\tinstance.name.strip() == u''\r\n\r\n\t# Or, generally speaking, RSS already sends us the title, \r\n\t# thus we should not do the job twice if we already have it.\r\n\tor (\r\n\t\tinstance.origin not in (\r\n\t\t\tORIGINS.FEEDPARSER,\r\n\t\t\tORIGINS.GOOGLE_READER,\r\n\t\t)\r\n\r\n\t\t# In case the origin is not one known to give us the title,\r\n\t\t# We should check if the title is not already extracted to \r\n\t\t# not do the job twice again.\r\n\r\n\t\tand (\r\n\t        instance.name.endswith(instance.url)\r\n\r\n\t        # Except if the user, the admin or the \r\n\t        # engine explicitely asks us to do so.\r\n\t        \r\n        \tor\r\n        \tforce\r\n        )\r\n\t) \r\n)", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 9, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-article-title-extract-accept-conditions", 
    "categories": [
      1, 
      4, 
      7
    ], 
    "name": "base accept conditions for article title extraction", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\n# This processor does NOT extract any title from anything.\r\n# It just defines a common accepts() conditions set that \r\n# other title extractor processors can start from.\r\n\r\nreturn True"
  }
},
{
  "pk": 15, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\nORIGINS = models.ORIGINS\r\n\r\nreturn (\r\n\t# If instance has no name currently, wherever its origin is, \r\n\t# it seems fine to try to find a name from any data we have\r\n\t# or can gather.\r\n\r\n\tinstance.name is None\r\n\tor \r\n\tinstance.name.strip() == u''\r\n\r\n\t# Or, generally speaking, RSS already sends us the title, \r\n\t# thus we should not do the job twice if we already have it.\r\n\tor (\r\n\t\tinstance.origin not in (\r\n\t\t\tORIGINS.FEEDPARSER,\r\n\t\t\tORIGINS.GOOGLE_READER,\r\n\t\t)\r\n\r\n\t\t# In case the origin is not one known to give us the title,\r\n\t\t# We should check if the title is not already extracted to \r\n\t\t# not do the job twice again.\r\n\r\n\t\tand (\r\n\t        instance.name.endswith(instance.url)\r\n\r\n\t        # Except if the user, the admin or the \r\n\t        # engine explicitely asks us to do so.\r\n\t        \r\n        \tor\r\n        \tforce\r\n        )\r\n\t) \r\n)", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 9, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": false, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-article-title-extract-accept-conditions", 
    "categories": [
      1, 
      4, 
      7
    ], 
    "name": "base accept conditions for article title extraction", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\n# This processor does NOT extract any title from anything.\r\n# It just defines a common accepts() conditions set that \r\n# other title extractor processors can start from.\r\n\r\nreturn True"
  }
},
{
  "pk": 16, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\nprepare_parameters = parameters.get('prepare', {})\r\n\r\nreturn (\r\n\r\n    # we work on articles\r\n    isinstance(instance, models.ContentItem)\r\n\r\n  \t# Content parsers accept only good articles.\r\n    # As of 20150208, ContentItem doesn't care,\r\n    # and for BaseItem & UrlItem it means already\r\n    # absolute and not duplicate, which is perfect.\r\n    and instance.is_good\r\n    \r\n    # and which are not already converted (or conversion is forced again).\r\n    and (\r\n    \tforce \r\n    \tor instance.content_type not in models.CONTENT_TYPES_FINAL\r\n    )\r\n\r\n  \tand prepare_parameters.get('use_excerpt_as_content', False)\r\n  \r\n    and bool(prepare_parameters.get('force_content_type', None))\r\n\r\n)\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 10, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-content-from-excerpt", 
    "categories": [
      1, 
      5
    ], 
    "name": "Fill item content from excerpt", 
    "short_description_en": "", 
    "parameters": "{'force_content_type': {'help_en': \"u'Set this to a valid content type. Usual values includes \\\\u201chtml\\\\u201d (most used) and \\\\u201cmarkdown\\\\u201d.'\", 'required': 'if use_excerpt_as_content is true', 'type': 'string', 'default_en': 'none'}, 'use_excerpt_as_content': {\"item excerpt will be duplicated into its content field. Setting this parameter requires setting \\\\u201cforce_content_type\\\\u201d too.'\": 'None', 'help_en': \"u'if true\", 'required': False, 'type': 'boolean', 'default_en': 'false; .'}}", 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\nprepare_parameters = parameters.get('prepare', {})\r\n\r\nCONTENT_TYPES = models.CONTENT_TYPES\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\nif prepare_parameters.get('use_excerpt_as_content', False):\r\n\r\n    force_content_type = prepare_parameters.get('force_content_type', None)\r\n\r\n    if force_content_type is None:\r\n        LOGGER.warning(u'excerpt_as_content: force_content_type is None, '\r\n                       u'aborting on %s %s.', isntance_name, instance_id)\r\n        return\r\n\r\n    if force_content_type.isdigit():\r\n        force_content_type = CONTENT_TYPES.index(int(force_content_type))\r\n\r\n    else:\r\n        force_content_type = force_content_type.upper()\r\n\r\n        try:\r\n            force_content_type = getattr(CONTENT_TYPES, force_content_type)\r\n\r\n        except AttributeError:\r\n            LOGGER.warning(u'excerpt_as_content: unknown content type %s, '\r\n                           u'aborting on %s %s.', force_content_type, \r\n                           isntance_name, instance_id)\r\n\r\n    # content_was_none = instance.content is None\r\n\r\n    instance.content = instance.excerpt\r\n    instance.content_type = force_content_type\r\n\r\n    if verbose:\r\n        LOGGER.info(u'excerpt_as_content: duplicated excerpt as %s content '\r\n                    u'for %s %s.', CONTENT_TYPES.symbolic(force_content_type),\r\n                    isntance_name, instance_id)\r\n\r\n    if commit:\r\n        #if content_was_none:\r\n        #    instance.save_without_historical_record()\r\n        #else:\r\n        # Worth a version.\r\n        instance.save()"
  }
},
{
  "pk": 16, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "", 
    "short_description_nt": "", 
    "accept_code": "\r\nprepare_parameters = parameters.get('prepare', {})\r\n\r\nreturn (\r\n\r\n    # we work on articles\r\n    isinstance(instance, models.ContentItem)\r\n\r\n  \t# Content parsers accept only good articles.\r\n    # As of 20150208, ContentItem doesn't care,\r\n    # and for BaseItem & UrlItem it means already\r\n    # absolute and not duplicate, which is perfect.\r\n    and instance.is_good\r\n    \r\n    # and which are not already converted (or conversion is forced again).\r\n    and (\r\n    \tforce \r\n    \tor instance.content_type not in models.CONTENT_TYPES_FINAL\r\n    )\r\n\r\n  \tand prepare_parameters.get('use_excerpt_as_content', False)\r\n  \r\n    and bool(prepare_parameters.get('force_content_type', None))\r\n\r\n)\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 10, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-content-from-excerpt", 
    "categories": [
      1, 
      5
    ], 
    "name": "Fill item content from excerpt", 
    "short_description_en": "", 
    "parameters": "{'force_content_type': {'help_en': \"u'Set this to a valid content type. Usual values includes \\\\u201chtml\\\\u201d (most used) and \\\\u201cmarkdown\\\\u201d.'\", 'required': 'if use_excerpt_as_content is true', 'type': 'string', 'default_en': 'none'}, 'use_excerpt_as_content': {\"item excerpt will be duplicated into its content field. Setting this parameter requires setting \\\\u201cforce_content_type\\\\u201d too.'\": 'None', 'help_en': \"u'if true\", 'required': False, 'type': 'boolean', 'default_en': 'false; .'}}", 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\nprepare_parameters = parameters.get('prepare', {})\r\n\r\nCONTENT_TYPES = models.CONTENT_TYPES\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\nif prepare_parameters.get('use_excerpt_as_content', False):\r\n\r\n    force_content_type = prepare_parameters.get('force_content_type', None)\r\n\r\n    if force_content_type is None:\r\n        LOGGER.warning(u'excerpt_as_content: force_content_type is None, '\r\n                       u'aborting on %s %s.', isntance_name, instance_id)\r\n        return\r\n\r\n    if force_content_type.isdigit():\r\n        force_content_type = CONTENT_TYPES.index(int(force_content_type))\r\n\r\n    else:\r\n        force_content_type = force_content_type.upper()\r\n\r\n        try:\r\n            force_content_type = getattr(CONTENT_TYPES, force_content_type)\r\n\r\n        except AttributeError:\r\n            LOGGER.warning(u'excerpt_as_content: unknown content type %s, '\r\n                           u'aborting on %s %s.', force_content_type, \r\n                           isntance_name, instance_id)\r\n\r\n    # content_was_none = instance.content is None\r\n\r\n    instance.content = instance.excerpt\r\n    instance.content_type = force_content_type\r\n\r\n    if verbose:\r\n        LOGGER.info(u'excerpt_as_content: duplicated excerpt as %s content '\r\n                    u'for %s %s.', CONTENT_TYPES.symbolic(force_content_type),\r\n                    isntance_name, instance_id)\r\n\r\n    if commit:\r\n        #if content_was_none:\r\n        #    instance.save_without_historical_record()\r\n        #else:\r\n        # Worth a version.\r\n        instance.save()"
  }
},
{
  "pk": 17, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "beautifulsoup4==4.3.2\r\n", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the title extractor common conditions\r\n\r\nreturn get_processor_by_slug(\r\n    '1fs-article-title-extract-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 11, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-html-title-extractor", 
    "categories": [
      1, 
      4
    ], 
    "name": "HTML title extractor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\nfrom bs4 import BeautifulSoup\r\nfrom django.utils.text import slugify\r\n\r\nCONTENT_TYPES = models.CONTENT_TYPES\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\ncontent = instance.content\r\n\r\nif instance.content_type != CONTENT_TYPES.HTML \\\r\n        or content is None or content.strip() == u'':\r\n\r\n    # Try to get non-cleaned HTML content from history.\r\n    \r\n    full_html_version = instance.history.filter(\r\n        content_type=CONTENT_TYPES.HTML).order_by('history_date').first()\r\n\r\n    if full_html_version is None:\r\n        LOGGER.warning(u'html-title-extractor: could not import title '\r\n                       u'of %s %s, which has no content and no past HTML '\r\n                       u'revision.', instance_name, instance_id)\r\n        return\r\n\r\n    content = full_html_version.content\r\n\r\nold_title = instance.name\r\n\r\ntry:\r\n    instance.name = BeautifulSoup(content).find(\r\n        'title').contents[0].strip()\r\n\r\nexcept:\r\n    LOGGER.exception(u'html-title-extractor: could not extract title '\r\n                     u'of %s %s via BeautifulSoup.',\r\n                     instance_name, instance_id)\r\n\r\nelse:\r\n    LOGGER.info(u'html-title-extractor: changed title of %s %s from '\r\n                u'\u201c%s\u201d to \u201c%s\u201d.', instance_name, instance_id,\r\n                old_title, instance.name)\r\n\r\n    instance.slug = slugify(instance.name)\r\n\r\nif commit:\r\n    # Do not waste a version, we didn't touch title if it was already \r\n    # set (or if force=True in which case \u201cnothing else matters\u2026\u201d). \r\n    instance.save_without_historical_record()\r\n"
  }
},
{
  "pk": 17, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "beautifulsoup4==4.3.2\r\n", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the title extractor common conditions\r\n\r\nreturn get_processor_by_slug(\r\n    '1fs-article-title-extract-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit)\r\n", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 11, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-html-title-extractor", 
    "categories": [
      1, 
      4
    ], 
    "name": "HTML title extractor", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "\r\nfrom bs4 import BeautifulSoup\r\nfrom django.utils.text import slugify\r\n\r\nCONTENT_TYPES = models.CONTENT_TYPES\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\n\r\ncontent = instance.content\r\n\r\nif instance.content_type != CONTENT_TYPES.HTML \\\r\n        or content is None or content.strip() == u'':\r\n\r\n    # Try to get non-cleaned HTML content from history.\r\n    \r\n    full_html_version = instance.history.filter(\r\n        content_type=CONTENT_TYPES.HTML).order_by('history_date').first()\r\n\r\n    if full_html_version is None:\r\n        LOGGER.warning(u'html-title-extractor: could not import title '\r\n                       u'of %s %s, which has no content and no past HTML '\r\n                       u'revision.', instance_name, instance_id)\r\n        return\r\n\r\n    content = full_html_version.content\r\n\r\nold_title = instance.name\r\n\r\ntry:\r\n    instance.name = BeautifulSoup(content).find(\r\n        'title').contents[0].strip()\r\n\r\nexcept:\r\n    LOGGER.exception(u'html-title-extractor: could not extract title '\r\n                     u'of %s %s via BeautifulSoup.',\r\n                     instance_name, instance_id)\r\n\r\nelse:\r\n    LOGGER.info(u'html-title-extractor: changed title of %s %s from '\r\n                u'\u201c%s\u201d to \u201c%s\u201d.', instance_name, instance_id,\r\n                old_title, instance.name)\r\n\r\n    instance.slug = slugify(instance.name)\r\n\r\nif commit:\r\n    # Do not waste a version, we didn't touch title if it was already \r\n    # set (or if force=True in which case \u201cnothing else matters\u2026\u201d). \r\n    instance.save_without_historical_record()\r\n"
  }
},
{
  "pk": 18, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "require::1fs-newspaper-common\r\n\r\n-e git+https://github.com/Karmak23/soup-strainer.git@c05006d2e8591909096b570da711d722aa903905#egg=strainer-master\r\nbreadability==0.1.20\r\nlxml==3.4.1\r\nhtml5lib==0.999\r\n", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the common base conditions, \r\n# plus we act only on full-HTML content.\r\n# Impossible to clean/convert anything other.\r\n\r\nreturn get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit\r\n\t) and instance.content_type == models.CONTENT_TYPES.HTML", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 12, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-multi-engines-automatic-html-cleaner", 
    "categories": [
      1, 
      8
    ], 
    "name": "Multi-engines automatic HTML cleaner", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "# legacy HTML cleaner processor\r\n\r\nimport gc\r\nimport logging\r\nimport strainer\r\nimport newspaper\r\nfrom breadability import readable\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\nsuccessfully_parsed = False\r\n\r\n#\r\n# NOTE: now that the downloader processor ensures unicode is written,\r\n#       after having eventually forced the encoding with a parameter,\r\n#       we don't have any mean to get back the original encoding, it\r\n#       was lost when the downloader finished. BUT now we can be sure\r\n#       that our own content is utf-8. If strainer failed to return\r\n#       a unicode string, at least should it be utf-8 encoded if it \r\n#       didn't ruin the contents.\r\n#\r\n#       Thus, we deliberately force it to utf-8.\r\n#\r\nencoding = 'utf-8'\r\ncontent = instance.content\r\n\r\nfor parser in ('lxml', 'html5lib', ):\r\n\r\n    STRAINER_EXTRACTOR = strainer.Strainer(parser=parser,\r\n                                           add_score=True)\r\n\r\n    # Strainer is bogus, it logs everything on 'root'\r\n    # logger, we cannot disable it selectively. Doomed.\r\n    logging.disable(logging.WARNING)\r\n\r\n    try:\r\n        # strainer expects an str, not unicode.\r\n        content = STRAINER_EXTRACTOR.feed(content.encode(encoding), \r\n                                          encoding=encoding)\r\n\r\n    except:\r\n        logging.disable(logging.NOTSET)\r\n        LOGGER.exception(u'multi-engine-automatic-cleaner: strainer '\r\n                         u'extraction [parser=%s] failed for %s %s', \r\n                         parser, instance_name, instance_id)\r\n    else:\r\n        successfully_parsed = True\r\n        logging.disable(logging.NOTSET)\r\n\r\n    del STRAINER_EXTRACTOR\r\n    gc.collect()\r\n\r\n    if successfully_parsed:\r\n        break\r\n\r\nif not successfully_parsed:\r\n\r\n    # Breadability logs too much, too.\r\n    logging.disable(logging.WARNING)\r\n\r\n    try:\r\n        breadability_article = readable.Article(\r\n            content, url=instance.url)\r\n        content = breadability_article.readable\r\n\r\n    except:\r\n        logging.disable(logging.NOTSET)\r\n        LOGGER.exception(u'multi-engine-automatic-cleaner: breadability '\r\n                         u'extraction failed for %s %s', \r\n                         instance_name, instance_id)\r\n    else:\r\n        successfully_parsed = True\r\n        logging.disable(logging.NOTSET)\r\n\r\nif not successfully_parsed:\r\n\r\n    try:\r\n        newspaper_article = newspaper.Article(url=instance.url)\r\n        newspaper_article.download()\r\n\r\n        # NOT HERE. NOT THE POINT.\r\n        # newspaper_article.parse()\r\n\r\n    except:\r\n            LOGGER.exception(u'multi-engine-automatic-cleaner: newspaper '\r\n                             u'extraction failed for %s %s', \r\n                             instance_name, instance_id)\r\n\r\n    else:\r\n        if newspaper_article.html.strip() != u'':\r\n            content = newspaper_article.html\r\n            successfully_parsed = True\r\n\r\nif not successfully_parsed:\r\n    LOGGER.error(u'multi-engine-automatic-cleaner: all cleaning methods '\r\n                 u'failed on %s %s, letting things in place.',\r\n                 instance_name, instance_id)\r\n    return\r\n\r\nif type(content) == type(unicode()):\r\n    # A modern tool already did the job.\r\n    instance.content = content\r\n\r\nelse:\r\n    # Strainer gives us a non-unicode boo-boo.\r\n    try:\r\n        instance.content = content.decode(\r\n            eventual_encoding=encoding)\r\n\r\n    except TypeError:\r\n        # Oops, data doesn't come from strainer\u2026\r\n        instance.content = content.decode(encoding=encoding)\r\n\r\ninstance.content_type = models.CONTENT_TYPES.CLEANED_HTML\r\n\r\n# The processing errors related to content are cleared by the chain run()\r\n# method. The next 3 lines should be obsolete one day, thus protected by\r\n# try/except block to not make the processor crash when the field is \r\n# removed from database.\r\ntry: \r\n    if instance.content_error:\r\n        statsd.gauge('articles.counts.content_errors', -1, delta=True)\r\n        instance.content_error = None\r\n\r\nexcept:\r\n    LOGGER.exception(u'multi-engine-automatic-cleaner: time to remove '\r\n                     u'the instance.content_error block?')\r\n\r\nif commit:\r\n    # This is worth a version in history to \r\n    # keep the non-cleaned content in case.\r\n    instance.save()\r\n\r\nwith statsd.pipeline() as spipe:\r\n    spipe.gauge('articles.counts.html', -1, delta=True)\r\n    spipe.gauge('articles.counts.cleaned', 1, delta=True)"
  }
},
{
  "pk": 18, 
  "model": "core.processor", 
  "fields": {
    "rght": 2, 
    "maintainer": null, 
    "duplicate_of": null, 
    "lft": 1, 
    "description_fr": "", 
    "requirements": "require::1fs-newspaper-common\r\n\r\n-e git+https://github.com/Karmak23/soup-strainer.git@c05006d2e8591909096b570da711d722aa903905#egg=strainer-master\r\nbreadability==0.1.20\r\nlxml==3.4.1\r\nhtml5lib==0.999\r\n", 
    "short_description_nt": "", 
    "accept_code": "\r\n# We use the common base conditions, \r\n# plus we act only on full-HTML content.\r\n# Impossible to clean/convert anything other.\r\n\r\nreturn get_processor_by_slug(\r\n    '1fs-article-parse-accept-conditions').accepts(\r\n        instance, force=force, verbose=verbose, commit=commit\r\n\t) and instance.content_type == models.CONTENT_TYPES.HTML", 
    "description_nt": "", 
    "languages": [], 
    "tree_id": 12, 
    "short_description_fr": "", 
    "source_address": "", 
    "parent": null, 
    "is_active": true, 
    "description_en": "", 
    "user": [
      "karmak23"
    ], 
    "slug": "1fs-multi-engines-automatic-html-cleaner", 
    "categories": [
      1, 
      8
    ], 
    "name": "Multi-engines automatic HTML cleaner", 
    "short_description_en": "", 
    "parameters": null, 
    "level": 0, 
    "duplicate_status": null, 
    "process_code": "# legacy HTML cleaner processor\r\n\r\nimport gc\r\nimport logging\r\nimport strainer\r\nimport newspaper\r\nfrom breadability import readable\r\n\r\ninstance_name = instance._meta.verbose_name\r\ninstance_id = instance.id\r\nsuccessfully_parsed = False\r\n\r\n#\r\n# NOTE: now that the downloader processor ensures unicode is written,\r\n#       after having eventually forced the encoding with a parameter,\r\n#       we don't have any mean to get back the original encoding, it\r\n#       was lost when the downloader finished. BUT now we can be sure\r\n#       that our own content is utf-8. If strainer failed to return\r\n#       a unicode string, at least should it be utf-8 encoded if it \r\n#       didn't ruin the contents.\r\n#\r\n#       Thus, we deliberately force it to utf-8.\r\n#\r\nencoding = 'utf-8'\r\ncontent = instance.content\r\n\r\nfor parser in ('lxml', 'html5lib', ):\r\n\r\n    STRAINER_EXTRACTOR = strainer.Strainer(parser=parser,\r\n                                           add_score=True)\r\n\r\n    # Strainer is bogus, it logs everything on 'root'\r\n    # logger, we cannot disable it selectively. Doomed.\r\n    logging.disable(logging.WARNING)\r\n\r\n    try:\r\n        # strainer expects an str, not unicode.\r\n        content = STRAINER_EXTRACTOR.feed(content.encode(encoding), \r\n                                          encoding=encoding)\r\n\r\n    except:\r\n        logging.disable(logging.NOTSET)\r\n        LOGGER.exception(u'multi-engine-automatic-cleaner: strainer '\r\n                         u'extraction [parser=%s] failed for %s %s', \r\n                         parser, instance_name, instance_id)\r\n    else:\r\n        successfully_parsed = True\r\n        logging.disable(logging.NOTSET)\r\n\r\n    del STRAINER_EXTRACTOR\r\n    gc.collect()\r\n\r\n    if successfully_parsed:\r\n        break\r\n\r\nif not successfully_parsed:\r\n\r\n    # Breadability logs too much, too.\r\n    logging.disable(logging.WARNING)\r\n\r\n    try:\r\n        breadability_article = readable.Article(\r\n            content, url=instance.url)\r\n        content = breadability_article.readable\r\n\r\n    except:\r\n        logging.disable(logging.NOTSET)\r\n        LOGGER.exception(u'multi-engine-automatic-cleaner: breadability '\r\n                         u'extraction failed for %s %s', \r\n                         instance_name, instance_id)\r\n    else:\r\n        successfully_parsed = True\r\n        logging.disable(logging.NOTSET)\r\n\r\nif not successfully_parsed:\r\n\r\n    try:\r\n        newspaper_article = newspaper.Article(url=instance.url)\r\n        newspaper_article.download()\r\n\r\n        # NOT HERE. NOT THE POINT.\r\n        # newspaper_article.parse()\r\n\r\n    except:\r\n            LOGGER.exception(u'multi-engine-automatic-cleaner: newspaper '\r\n                             u'extraction failed for %s %s', \r\n                             instance_name, instance_id)\r\n\r\n    else:\r\n        if newspaper_article.html.strip() != u'':\r\n            content = newspaper_article.html\r\n            successfully_parsed = True\r\n\r\nif not successfully_parsed:\r\n    LOGGER.error(u'multi-engine-automatic-cleaner: all cleaning methods '\r\n                 u'failed on %s %s, letting things in place.',\r\n                 instance_name, instance_id)\r\n    return\r\n\r\nif type(content) == type(unicode()):\r\n    # A modern tool already did the job.\r\n    instance.content = content\r\n\r\nelse:\r\n    # Strainer gives us a non-unicode boo-boo.\r\n    try:\r\n        instance.content = content.decode(\r\n            eventual_encoding=encoding)\r\n\r\n    except TypeError:\r\n        # Oops, data doesn't come from strainer\u2026\r\n        instance.content = content.decode(encoding=encoding)\r\n\r\ninstance.content_type = models.CONTENT_TYPES.CLEANED_HTML\r\n\r\n# The processing errors related to content are cleared by the chain run()\r\n# method. The next 3 lines should be obsolete one day, thus protected by\r\n# try/except block to not make the processor crash when the field is \r\n# removed from database.\r\ntry: \r\n    if instance.content_error:\r\n        statsd.gauge('articles.counts.content_errors', -1, delta=True)\r\n        instance.content_error = None\r\n\r\nexcept:\r\n    LOGGER.exception(u'multi-engine-automatic-cleaner: time to remove '\r\n                     u'the instance.content_error block?')\r\n\r\nif commit:\r\n    # This is worth a version in history to \r\n    # keep the non-cleaned content in case.\r\n    instance.save()\r\n\r\nwith statsd.pipeline() as spipe:\r\n    spipe.gauge('articles.counts.html', -1, delta=True)\r\n    spipe.gauge('articles.counts.cleaned', 1, delta=True)"
  }
},
{
  "pk": 5, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 5, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-article-default-forced-utf8", 
    "categories": [
      1, 
      2, 
      4
    ], 
    "name": "articles default \u2014 HTML forced to utf-8"
  }
},
{
  "pk": 5, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 5, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-article-default-forced-utf8", 
    "categories": [
      1, 
      2, 
      4
    ], 
    "name": "articles default \u2014 HTML forced to utf-8"
  }
},
{
  "pk": 5, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 5, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-article-default-forced-utf8", 
    "categories": [
      1, 
      2, 
      4
    ], 
    "name": "articles default \u2014 HTML forced to utf-8"
  }
},
{
  "pk": 6, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 6, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1ft-article-default-download-html-only", 
    "categories": [
      1, 
      3
    ], 
    "name": "test article download HTML only"
  }
},
{
  "pk": 6, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 6, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1ft-article-default-download-html-only", 
    "categories": [
      1, 
      3
    ], 
    "name": "test article download HTML only"
  }
},
{
  "pk": 4, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 4, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1ft-opengraph", 
    "categories": [
      1, 
      3
    ], 
    "name": "test OpenGraph extractor"
  }
},
{
  "pk": 4, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 4, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1ft-opengraph", 
    "categories": [
      1, 
      3
    ], 
    "name": "test OpenGraph extractor"
  }
},
{
  "pk": 1, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "The historical 1flow processing chain, just ported as a dynamic chain.", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 1, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-legacy-article-parsers", 
    "categories": [
      1, 
      2, 
      4, 
      6, 
      8
    ], 
    "name": "legacy article parsers"
  }
},
{
  "pk": 1, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "The historical 1flow processing chain, just ported as a dynamic chain.", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 1, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-legacy-article-parsers", 
    "categories": [
      1, 
      2, 
      4, 
      6, 
      8
    ], 
    "name": "legacy article parsers"
  }
},
{
  "pk": 1, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "The historical 1flow processing chain, just ported as a dynamic chain.", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 1, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-legacy-article-parsers", 
    "categories": [
      1, 
      2, 
      4, 
      6, 
      8
    ], 
    "name": "legacy article parsers"
  }
},
{
  "pk": 1, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "The historical 1flow processing chain, just ported as a dynamic chain.", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 1, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-legacy-article-parsers", 
    "categories": [
      1, 
      2, 
      4, 
      6, 
      8
    ], 
    "name": "legacy article parsers"
  }
},
{
  "pk": 1, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "The historical 1flow processing chain, just ported as a dynamic chain.", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 1, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-legacy-article-parsers", 
    "categories": [
      1, 
      2, 
      4, 
      6, 
      8
    ], 
    "name": "legacy article parsers"
  }
},
{
  "pk": 3, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 3, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-full-newspaper-only", 
    "categories": [
      1, 
      2, 
      4, 
      6, 
      8
    ], 
    "name": "full newspaper-only"
  }
},
{
  "pk": 3, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 3, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-full-newspaper-only", 
    "categories": [
      1, 
      2, 
      4, 
      6, 
      8
    ], 
    "name": "full newspaper-only"
  }
},
{
  "pk": 3, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 3, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-full-newspaper-only", 
    "categories": [
      1, 
      2, 
      4, 
      6, 
      8
    ], 
    "name": "full newspaper-only"
  }
},
{
  "pk": 3, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 3, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-full-newspaper-only", 
    "categories": [
      1, 
      2, 
      4, 
      6, 
      8
    ], 
    "name": "full newspaper-only"
  }
},
{
  "pk": 3, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 3, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-full-newspaper-only", 
    "categories": [
      1, 
      2, 
      4, 
      6, 
      8
    ], 
    "name": "full newspaper-only"
  }
},
{
  "pk": 9, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 8, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-article-default-excerpt-to-cleaned-html-content-no-metadata", 
    "categories": [
      1, 
      2, 
      4
    ], 
    "name": "articles default \u2014 excerpt to cleaned HTML content \u2014 no metadata"
  }
},
{
  "pk": 9, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 8, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-article-default-excerpt-to-cleaned-html-content-no-metadata", 
    "categories": [
      1, 
      2, 
      4
    ], 
    "name": "articles default \u2014 excerpt to cleaned HTML content \u2014 no metadata"
  }
},
{
  "pk": 9, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 8, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-article-default-excerpt-to-cleaned-html-content-no-metadata", 
    "categories": [
      1, 
      2, 
      4
    ], 
    "name": "articles default \u2014 excerpt to cleaned HTML content \u2014 no metadata"
  }
},
{
  "pk": 2, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "Articles default parsing chain", 
    "rght": 2, 
    "applies_on": [
      [
        "core", 
        "article"
      ]
    ], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 2, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1fs-article-default", 
    "categories": [
      1
    ], 
    "name": "articles default"
  }
},
{
  "pk": 8, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 7, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1ft-excerpt-as-content", 
    "categories": [
      1, 
      3
    ], 
    "name": "test excerpt as content"
  }
},
{
  "pk": 8, 
  "model": "core.processingchain", 
  "fields": {
    "short_description_en": "", 
    "rght": 2, 
    "applies_on": [], 
    "duplicate_of": null, 
    "parent": null, 
    "level": 0, 
    "description_nt": "", 
    "short_description_nt": "", 
    "is_active": true, 
    "description_en": "", 
    "languages": [], 
    "lft": 1, 
    "description_fr": "", 
    "user": [
      "karmak23"
    ], 
    "tree_id": 7, 
    "short_description_fr": "", 
    "duplicate_status": null, 
    "slug": "1ft-excerpt-as-content", 
    "categories": [
      1, 
      3
    ], 
    "name": "test excerpt as content"
  }
},
{
  "pk": 14, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-legacy-article-parsers"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": "", 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 16, 
    "position": 0, 
    "notes_en": ""
  }
},
{
  "pk": 4, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-legacy-article-parsers"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": "", 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 1, 
    "position": 1, 
    "notes_en": ""
  }
},
{
  "pk": 17, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-legacy-article-parsers"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": null, 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 17, 
    "position": 2, 
    "notes_en": ""
  }
},
{
  "pk": 3, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-legacy-article-parsers"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": "", 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 2, 
    "position": 3, 
    "notes_en": ""
  }
},
{
  "pk": 8, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-legacy-article-parsers"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": "", 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 14, 
    "position": 4, 
    "notes_en": ""
  }
},
{
  "pk": 18, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-legacy-article-parsers"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": null, 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 18, 
    "position": 5, 
    "notes_en": ""
  }
},
{
  "pk": 5, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-legacy-article-parsers"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": "", 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 3, 
    "position": 6, 
    "notes_en": ""
  }
},
{
  "pk": 2, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-article-default"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": "", 
    "item_type": [
      "core", 
      "processingchain"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 1, 
    "position": 0, 
    "notes_en": ""
  }
},
{
  "pk": 6, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-full-newspaper-only"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": "", 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 9, 
    "position": 0, 
    "notes_en": ""
  }
},
{
  "pk": 9, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1ft-opengraph"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": null, 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 1, 
    "position": 0, 
    "notes_en": ""
  }
},
{
  "pk": 10, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1ft-opengraph"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": null, 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 14, 
    "position": 1, 
    "notes_en": ""
  }
},
{
  "pk": 11, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-article-default-forced-utf8"
    ], 
    "parameters": "{'download': {'force_encoding': 'utf-8'}}", 
    "notes_fr": "Teste la cha\u00eene de traitement par d\u00e9faut en for\u00e7ant le d\u00e9codage du contenu en utf-8, ne tenant pas compte de ce que le site d\u00e9clare dans ses en-t\u00eates.", 
    "is_active": true, 
    "check_error": null, 
    "item_type": [
      "core", 
      "processingchain"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 2, 
    "position": 0, 
    "notes_en": ""
  }
},
{
  "pk": 12, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1ft-article-default-download-html-only"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": null, 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 1, 
    "position": 0, 
    "notes_en": ""
  }
},
{
  "pk": 15, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1ft-excerpt-as-content"
    ], 
    "parameters": null, 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": null, 
    "item_type": [
      "core", 
      "processor"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 16, 
    "position": 0, 
    "notes_en": ""
  }
},
{
  "pk": 16, 
  "model": "core.chaineditem", 
  "fields": {
    "chain": [
      "1fs-article-default-excerpt-to-cleaned-html-content-no-metadata"
    ], 
    "parameters": "{'process_metadata': False, 'prepare': {'force_content_type': 'CLEANED_HTML', 'use_excerpt_as_content': True}}", 
    "notes_fr": "", 
    "is_active": true, 
    "check_error": null, 
    "item_type": [
      "core", 
      "processingchain"
    ], 
    "is_valid": true, 
    "notes_nt": "", 
    "item_id": 2, 
    "position": 0, 
    "notes_en": ""
  }
}
]
